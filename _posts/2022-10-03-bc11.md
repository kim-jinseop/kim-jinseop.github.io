---
layout: single
title:  "Week3-day1"
toc: true
toc_sticky: true

use_math: true
---

딥러닝 기본

<br>

## [ special ]
- 

## 1. Historical Review

### key Components
- `data` 
- `model` 
- `loss` 
- `algorithm`

### History
- 2012 : AlexNet
- 2013 : DQN
- 2014 : Encode/Decoder, Adam
- 2015 : GAN, ResNet
- 2017 : Transformer
- 2018 : Bert
- 2019 : Big Language Models(GPT-x)
- 2020 : Self-Supervised Learning

<br>

## 2. Neural Networks & Multi-Layer Perceptron

### Linear Neural Networks

- Data : \\( D = [(x_i, y_i)]_{i=1}^N \\)
- Model : \\( \hat y = wx + b \\)
- Loss : \\( loss = \frac{1}{N}\sum_{i=1}^N(y_i - \hat y_i)^2 \\)

- w와 b를 찾는 방법
    - \\( \frac{\sigma loss}{\sigma b} = \frac{\sigma}{\sigma b}\frac{1}{N}\sum_{i=1}^N(y_i - \hat y_i)^2 \\)
    - \\( = \frac{\sigma}{\sigma b}\frac{1}{N}\sum_{i=1}^N(y_i - wx_i - b)^2 \\)
    - \\( = -\frac{1}{N}\sum_{i=1}^N-2(y_i - wx_i - b) \\)  
        - \\( w \leftarrow w - \eta\frac{\sigma loss}{\sigma w} \\)
        - \\( b \leftarrow b - \eta\frac{\sigma loss}{\sigma b} \\)
        - \\( \eta = stepsize\\) 
        
### Beyond Linear Neural Network
- Activation functions
    - ReLU(Rectified Linear Unit)
    - Sigmoid
    - Hyperbolic Tangent
    
### Multi-layer Perceptron
- Regression Task
- Classfication Task
- Probabilistic Task
