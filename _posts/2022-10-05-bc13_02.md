---
layout: single
title:  "Week3-day3_2"
toc: true
toc_sticky: true

use_math: true
---

DL Basic - RNN

<br>

## [ special ]
-

## 1. Recurrent Neural Networks
- 단점 : Short-term dependencies

### Sequential Model
- 중간의 Hidden state가 과거의 정보를 요약
- 다음번 time step은 Hidden state하나에만 dependent

### LSTM(Long Short Term Memory)

![ screen_lstm](https://user-images.githubusercontent.com/93374348/193984319-66dfe458-c790-4be4-bcc4-6d8dc58e63f5.png)

- Forget Gate : Decide which information to `throw` away  
    - \\( f_t = \sigma(W_f[h_{t-1}, x_t] + b_f) \\)       

- Input Gate : Decide which information to `store` in the cell state  
    - \\( i_t = \sigma(W_i[h_{t-1}, x_t] + b_i) \\)
    - \\( \tilde C_t = tanh(W_C[h_{t-1}, x_t] + b_C) \\)  
    
- Update cell : Update the cell state
    - \\( i_t = \sigma(W_i[h_{t-1}, x_t] + b_i) \\)
    - \\( C_t = f_t * C_{t-1} + i_t * \tilde C_t \\)  
    
- Output Gate : Make output using the updated cell state 
    - \\( o_t = \sigma(W_o[h_{t-1}, x_t] + b_o) \\)
    - \\( h_t = o_t * tanh(C_t) \\)  
    
### GRU(Gated Recurrent Unit)
- two gates (reset gate and update gate)
- No cell state, just hidden state

<br>

## 2. Transformer
- 재귀가 아닌 어텐션이라고 불리는 구조를 활용
- 장점 : flexible하고 많은걸 표현할 수 있는 네트워크를 만들수 있다
- encoders and decoders are stacked
- encoder
     - self_Attention
         - x_n(vector) -> z_n 로 갈때 모든 x_n의 paths have dependencies 
         - 3개의 Vector(각 입력마다) : `Queries, keys, values`
         - score vector : Encoding 하고자 하는 target의 Queries Vactor와 나머지 모든 n개의 대한 key vactor를 내적 - 관계를 확인
         - score이 생기면 softmax
         - the final encoding is done by the weighted sum of the value vectors
         
     - Feed Forward Neural Network
     
- Encoder -> Decoder로 이동할때 key와 value를 전달
