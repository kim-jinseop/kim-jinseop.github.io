---
layout: single
title:  "Week2-day1"
toc: true
toc_sticky: true

use_math: true
---

PyTorch

<br>

## 1. Introduction to PyTorch

### Computational graphs used
- TensorFlow : Static graphs (Define and Run)
    - 그래프를 먼저 정의 -> 실행시점에 데이터 feed
- PyTorch : Dynamic computation graphs (Define by Run)
    - 실행을 하면서 그래프를 생성하는 방식
    - 장점 :
        - 즉시 확인 가능
        - 자동미분 지원 DL연산을 지원
        - Numpy 구조를 가지는 Tensor 객체로 array 표현
    - Numpy + AutoGrad + Function
    
<br>

## 2. PyTorch Basics

### Tensor
- 다차원 Arrays를 표현하는 PyTorch 클래스


```python
import numpy as np
n_arr = np.arange(10).reshape(2,5)
print(n_arr)
print(n_arr.ndim, n_arr.shape)
```

    [[0 1 2 3 4]
     [5 6 7 8 9]]
    2 (2, 5)



```python
import torch
t_arr = torch.FloatTensor(n_arr)
print(t_arr)
print(t_arr.ndim, t_arr.shape)
```

    tensor([[0., 1., 2., 3., 4.],
            [5., 6., 7., 8., 9.]])
    2 torch.Size([2, 5])



```python
# data to tensor
data = [[3,5], [10,5]]
x = torch.tensor(data)
x
```




    tensor([[ 3,  5],
            [10,  5]])




```python
# npArray to Tensor
np_arr = np.array(data)
tx = torch.from_numpy(np_arr)
tx
```




    tensor([[ 3,  5],
            [10,  5]])



### numpy like operations
- pytorch의 tensor는 gpu에 올려서 사용가능


```python
x.device
```




    device(type='cpu')




```python
print(torch.cuda.device_count())
```

    0



```python
if torch.cuda.is_available() :
    x_cuda = x.to('cuda')
x_cuda.device

# GPU가 없어서 사용불가
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    Input In [69], in <cell line: 3>()
          1 if torch.cuda.is_available() :
          2     x_cuda = x.to('cuda')
    ----> 3 x_cuda.device


    NameError: name 'x_cuda' is not defined


### Tensor handling
- view : reshape과 동일하게 tensor의 shpae을 변환
- squeeze : 차원의 개수가 1인 차원을 삭제 (압축)
- unsqueeze : 차원의 개수가 1인 차원을 추가


```python
t_ex = torch.rand(size=(2,3,2))
t_ex
```




    tensor([[[0.7710, 0.4842],
             [0.0819, 0.1590],
             [0.3341, 0.2295]],
    
            [[0.0497, 0.8843],
             [0.6508, 0.0361],
             [0.5725, 0.8882]]])




```python
t_ex.view([-1,6])
```




    tensor([[0.7710, 0.4842, 0.0819, 0.1590, 0.3341, 0.2295],
            [0.0497, 0.8843, 0.6508, 0.0361, 0.5725, 0.8882]])




```python
print(t_ex.squeeze().shape)
print(t_ex.unsqueeze(0).shape)
print(t_ex.unsqueeze(1).shape)
print(t_ex.unsqueeze(2).shape)
```

    torch.Size([2, 3, 2])
    torch.Size([1, 2, 3, 2])
    torch.Size([2, 1, 3, 2])
    torch.Size([2, 3, 1, 2])


- 기본적인 tensor의 operations는 numpy와 동일


```python
n1 = np.arange(10).reshape(2,5)
t1 = torch.FloatTensor(n1)

print(t1 + t1)
print(t1 - t1)
print(t1 + 5)
```

    tensor([[ 0.,  2.,  4.,  6.,  8.],
            [10., 12., 14., 16., 18.]])
    tensor([[0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.]])
    tensor([[ 5.,  6.,  7.,  8.,  9.],
            [10., 11., 12., 13., 14.]])


- 행렬곱셈 연산 : dot이 아닌 mm


```python
n2 = np.arange(10).reshape(5,2)
t2 = torch.FloatTensor(n2)

print(t1.mm(t2))
print(t1.matmul(t2))
print(t1.dot(t2)) # 스칼라, 벡터 연산에서 사용가능 
```

    tensor([[ 60.,  70.],
            [160., 195.]])
    tensor([[ 60.,  70.],
            [160., 195.]])



    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    Input In [74], in <cell line: 6>()
          4 print(t1.mm(t2))
          5 print(t1.matmul(t2))
    ----> 6 print(t1.dot(t2))


    RuntimeError: 1D tensors expected, but got 2D and 2D tensors


- mm과 matmul은 broadcasting 지원 차이


```python
# 사용 주의
a = torch.rand(5,2,3)
b = torch.rand(3)

print(a.matmul(b))
print(a.dot(b)) # broadcasting 지원하지 않음 
```

    tensor([[0.8617, 1.1221],
            [0.8950, 0.6537],
            [0.8632, 0.3736],
            [0.8633, 0.8747],
            [1.3392, 1.0098]])



    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    Input In [75], in <cell line: 6>()
          3 b = torch.rand(3)
          5 print(a.matmul(b))
    ----> 6 print(a.dot(b))


    RuntimeError: 1D tensors expected, but got 3D and 1D tensors


### AutoGrad
- pyTorch의 핵심은 자동 미분의 지원 -> backward 함수 사용

$$
y = w^2 \\ 
z = 10*y + 25 \\
z = 10*w^2 + 25 
$$


```python
w = torch.tensor(2.0, requires_grad=True) # 미분대상에 true 
y = w**2
z = 10*y+25
z.backward()
w.grad
```




    tensor(40.)



- 편미분
$$ Q = 3a^3 - b^2  $$


```python
a = torch.tensor([2.,3.], requires_grad = True)
b = torch.tensor([6.,4.], requires_grad = True)
Q = 3*a**3 - b**2
external_grad = torch.tensor([1.,1.])
Q.backward(gradient = external_grad)

print(Q)
print(a.grad)
print(b.grad)
```

    tensor([-12.,  65.], grad_fn=<SubBackward0>)
    tensor([36., 81.])
    tensor([-12.,  -8.])

