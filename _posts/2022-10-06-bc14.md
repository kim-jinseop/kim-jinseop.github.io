---
layout: single
title:  "Week3-day4"
toc: true
toc_sticky: true

use_math: true
---

DL Basic - Generative Model

<br>

## [ special ]
-

## 1. Generative Models Part 1

### Learing a Generative Model
- Generation : \\( \hat x ~ p(x), \hat x \\) should like a dog (ex)
- Density estimation : \\( p(x) \\) should be high , and low otherwise

### Basic Discrete Distributions
- Bernoulli distribution : ex) coin flip
- Categorical distribution : ex) m-sided dice

### Conditional Independence
- chain rule : \\( P(X_1,....,X_n) = P(X_1)P(X_2;X_1)P(X_3;X_1,X_2)...P(X_n;X_1,...,X_{n-1}) \\)
- parameters : 
    - \\( P(X_1) \\) : 1 parameter 
    - \\( P(X_2;X_1) \\) : 2 parameter (\\( P(X_2;X_1=0), P(X_2;X_1=1) \\))  
    - \\( P(X_3;X_1,X_2) \\) : 4 parameter  
    - total: \\( 1+2+2^2+…2^{n−1}=2^n−1 \\)     
<br>   

- Markov assumption 적용하면 -> 2n-1

### Autoregressive Models
- ordering이 필요
- summary of autoregressive Models
    - Easy to sample
    - Easy to compute probability
    - Easy to extended to continuous
    
<br>
   
## 2. Generative Models Part 2

### Maximum Likelihood Learning
- KL-divergence
- Empirical Risk Minimization

### Latent Variable Models

#### Variational Autoencoder
- generate 할 수 있는 확률분포를 찾는 것이 목적
- Variational inference
    - Posterior distribution
    - Variational distribution 
    
    - \\( \log p_{\theta}(x) \\) -> Maximum Likehood Learning up
        - \\( = \mathbb E_{q_{\phi}(z \vert x)} [\log \frac{p_{\theta}(x,z)}{q_{\phi}(z \vert x)}] + D_{KL} \Big( q_{\phi} (z \vert x) \Vert p_{\theta}(z \vert x) \Big) \\)
        - \\( = \text{ ELBO } + \text{ Variational Gap} \\)
        - -> 목적은 ELBO up
<br><br>
- Evidence Lower Bound
    - \\(\mathbb E_{q_{\phi}(z \vert x)} [\log \frac{p_{\theta}(x,z)}{q_{\phi}(z \vert x)}] = \int \log \frac{p_{\theta}(x \vert z) p(z)}{q_{\phi}(z \vert x)} q_{\phi}(z \vert x) dz \\)
    - \\(= \mathbb E_{q_{\phi}(z \vert x)} [q_{\phi}(x \vert z)] - D_{KL} \Big( q_{\phi} (z \vert x) \Vert p(z) \Big) \\)
    - \\( \text{ ELBO } = \text{ Reconstruction term } - \text{ Prior fitting term } \\)

### Generative Adversarial  Networks (GAN)
- two player minimax game between generator and discriminator
    - two Jenson-Shannon Divergence
    
### Diffusion Models
- noise로 부터 image 생성
- Forward process : progressively injects noise to and image
- reverse process : learned in such a way to denoise the perturbed image back to a clean image

### DALL-E2
- Text -> image
- 특정영역의 이미지를 변경할 수 있다. (주변 상황에 맞춰)
