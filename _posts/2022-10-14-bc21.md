---
layout: single
title:  "Week4-day5"
toc: true
toc_sticky: true

use_math: true
---

CV - CNN Visualization

<br>

## [ special ]


# - Visualizing CNN
- debug와 비슷한 형태를 가지는 역할
- Filter visualization

# - Analysis of model behaviors
- Nearest neighbors
    - database 
    
- Dimensionality reduction 차원축소
    - t-SNE
    
# - Model decision explanation
- 해석

## 1. Saliency test 1
- Occlusion map

## 2. Saliency test 2
- via Backpropagation 
    - get a class score or th taget source image 
    - Backpropagate
    - Visualize the obtained gradient magnitude map
    
## 3. Class activation mapping
- CAM (Class activation mapping)
    - Global average pooling (GAP)
- Grad-CAM 

- SCOUTER
    - target이 왜 아닌지도 판단가능
    
<br>

# - AutoGrad


```python
import torch
```


```python
x = torch.randn(2, requires_grad = True)
y = x * 3
gradients = torch.tensor([100, 0.1], dtype=torch.float)
y.backward(gradients,retain_graph = True) # retain_graph = True 로 하게 되면 backward 중복사용가능
print(x.grad)
```

    tensor([300.0000,   0.3000])



```python
y.backward(gradients) # Gradients are accumulated
print(x.grad)
```

    tensor([600.0000,   0.6000])


## Autograd - grad_fn


```python
x = torch.randn(2, requires_grad = True)
y = x * 3
z = x / 2
w = x + y
w,y,z
```




    (tensor([2.0730, 4.8206], grad_fn=<AddBackward0>),
     tensor([1.5547, 3.6154], grad_fn=<MulBackward0>),
     tensor([0.2591, 0.6026], grad_fn=<DivBackward0>))



## Autograd - hook: register_forward_hook
- function을 hooking 할 수 있다.


```python
import torch.nn as nn
import torch.nn.functional as F
```


```python
class SimpleNet(nn.Module) :
    def __init__(self) :
        super(SimpleNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, 5)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(10, 20, 5)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.fc = nn.Linear(320, 50)
        self.out = nn.Linear(50, 10)
        
    def forward(self, input) :
        x = self.pool1(F.relu(self.conv1(input)))
        x = self.pool2(F.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc(x))
        x = F.relu(self.out(x))
        return x
    
def hook_func(self, input, output) :
    print('inside' + self.__class__.__name__ + 'forward')
    print('')
    print('input:', type(input))
    print('input[0]:', type(input[0]))
    print('output:', type(output))
    print('')
```


```python
net = SimpleNet()
```


```python
net.conv1.register_forward_hook(hook_func)
```




    <torch.utils.hooks.RemovableHandle at 0x12582ea60>




```python
net.conv2.register_forward_hook(hook_func)
```




    <torch.utils.hooks.RemovableHandle at 0x12582efd0>




```python
input = torch.randn(1,1,28,28)
out = net(input)
```

    insideConv2dforward
    
    input: <class 'tuple'>
    input[0]: <class 'torch.Tensor'>
    output: <class 'torch.Tensor'>
    
    insideConv2dforward
    
    input: <class 'tuple'>
    input[0]: <class 'torch.Tensor'>
    output: <class 'torch.Tensor'>
    



```python
def hook_pre(self, input) :
    print('inside' + self.__class__.__name__ + 'forward')
    print('')
    print('input:', type(input))
    print('input[0]:', type(input[0]))
```


```python
net = SimpleNet()
net.conv1.register_forward_pre_hook(hook_pre)

input = torch.randn(1,1,28,28)
out = net(input)
```

    insideConv2dforward
    
    input: <class 'tuple'>
    input[0]: <class 'torch.Tensor'>


## Autograd - hook: register_backward_hook


```python
def hook_grad(self, grad_input, grad_output) :
    print('inside' + self.__class__.__name__ + ' backward')
    print('inside class' + self.__class__.__name__)
    print('')
    print('grad_input:', type(grad_input))
    print('grad_input[0]:', type(grad_input[0]))
    print('grad_output:', type(grad_output))
    print('grad_output[0]:', type(grad_output[0]))
    print('')
```


```python
net = SimpleNet()
net.conv1.register_backward_hook(hook_grad)

input = torch.randn(1,1,28,28)
out = net(input)

target = torch.tensor([3], dtype=torch.long)
loss_fn = nn.CrossEntropyLoss()
err = loss_fn(out, target)
err.backward()
```

    insideConv2d backward
    inside classConv2d
    
    grad_input: <class 'tuple'>
    grad_input[0]: <class 'NoneType'>
    grad_output: <class 'tuple'>
    grad_output[0]: <class 'torch.Tensor'>
    


## Autograd - remove


```python
net = SimpleNet()
h = net.conv1.register_forward_hook(hook_func)
input = torch.randn(1,1,28,28)
out = net(input)
```

    insideConv2dforward
    
    input: <class 'tuple'>
    input[0]: <class 'torch.Tensor'>
    output: <class 'torch.Tensor'>
    



```python
h.remove() #등록된 hook을 삭제
out = net(input)
```
