---
layout: single
title:  "Week3-day2_1"
toc: true
toc_sticky: true

use_math: true
---

DL Basic - 최적화

<br>

## [ special ]
- 같은 input units에 연결된 서로 다른 hidden units들에 대해, 파라미터들의 초기값을 서로 다르게 초기화해주어야하며, 이를 `symmetry breaking`이라 한다.

## 1. Optimization

### Generalization
- Overfitting : 학습데이터 동작은 잘되지만 테스트데이터에 동작하지 않는 것 
- Underfitting : 테스트데이터 동작은 잘되지만 학습데이터에 동작하지 않는 것

### Cross - validation
- test는 어떤식으로든 활용하면 안된다.
- train 데이터를 k개로 나누고 일부를 test로 사용

### Bias and Variance
- Bias : target과의 거리
- Variance : 밀집도와 유사

### Bagging vs Boosting


### Gradient Descent Methods
- Stochastic gradient descent
- Mini-batch gradient descent -> 대부분 사용
- Batch gradient descent

- `Batch-size Matters`

- optm
    - Gradient descent
    - Momentum (\\( \beta \\))
    - Nesterov Accelerate
    - Adagrad
    - Adadelta
    - RMSprop
    - Adam
    
### Regulariztion
- Early stopping
    - Validation error를 활용
- Parmeter norm penalty
    - 뉴럴네트워크 파라미터를 너무 커지지않게 한다
- Data Augmentaion
    - lable이 변경되지 않는 조건하에 이미지를 변환
- Noise Robustness
- Lable Smooting
    - Mix-up
    - CutMix
- Dropout
- Batch Normalization
