---
layout: single
title:  "Week2-day4"
toc: true
toc_sticky: true

use_math: true
---

PyTorch

<br>

## [ special ]
```
boostcamp 과제를 해결하면서 모르는 내용이 너무 많았다.
몰랐던 내용을 정리할 필요를 느껴 해당 라인을 추가하게 됐다. 
(많은 동료와 팀원들에게 감사하다는 말씀 드리고 싶습니다 ㅠ)
```
- tokenizing : 문장에서 단어 추출 -> 고유한 단어마다 고유한 ID를 부여하는 과정
- encoding :단어 -> 고유한 ID로 변환하는 과정
- decoding :고유한 ID -> 단어로 변환하는 과정



## 1. Multi-GPU

### Model parallel
- 다중 GPU에 학습을 분산하는 두 가지 방법
    - 모델을 나누기 / 데이터를 나누기
- 모델을 나누는 것은 생각보다 예전부터 썼음
- 모델의 병목, 파이프라인의 어려움 등으로 인해 모델 병렬화는 고난이도 과제
- 데이터를 나눠 GPU에 할당후 결과와 평균을 취하는 방법
- minibatch 수식과 유사한데 한번에 여러 GPU에서 수행

- DataParallel : 단순히 데이터를 분배한후 평균을 취함
- DistributedDataParallel : 각 cpu마다 process를 생성하여 개별 gpu 에 할당

<br>

## 2. Hyperparameter Tuning
- 모델 스스로 학습하지 않는 값은 사람이 지정 (learing rate, 모델의 크기, optimizer 등)

### 가장 기본적인 방법
- gird vs random
- 최근에는 베이지안 기반 기법들이 주도

### Ray
- multi-node multi processing 지원 모듈
- ML/DL의 병렬처리를 위해 개발된 모듈
- 기존적으로 현재의 분산병렬 ML/DL 모듈의 표준
- Hyperperameter Search를 위한 다양한 모듈 제공
