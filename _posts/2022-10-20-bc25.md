---
layout: single
title:  "Week5-day4"
toc: true
toc_sticky: true

use_math: true
---

CV - Recent Trends on Vision Transformer
<br>


# - Transformers
- No recurrence 
- No convolution
- attention!

### Neural NLP history
- Recurrent Neural NetWorks(RNNs)
    - RNNs
    - Bi-directional RNNs
    
### Self-attention
- Query, key, and value
- Positional encoding

### Architecture
- Encoder
    - Multi-head self-attention (MHA)
    - Fully connected feed-forward network (FFN)
    
- Decoder
    - Masked multi-head attention
    - or multi-head attention for self-attention
    
    
# - Vision Transformers (ViTs)
### Architecture
- Split an image into fixed-size patches
- Linearly embed each patch
- Add positional embeddings
- Transformer encoder
- Classifiaction token

### Appliaction 1 : object detection
- DETR : End-to-End Object Detection with Transformers
- Method 
    - Backbone is conventional CNNs
    - Transformer encoder
    - Transformer decoder
    - Prediction heads (feed forward network; FFN)
    - Bipartite matching 
    
### Application 2 : semantic segmentation
- MaskFormer : Per-pixel Classification is Nat all You Need for Sementic Segmentation
- Method
    - Pixel=level module
    - Transformer module
    - Segmentation module - classification
    - Segmentation moduel - mask prediction
    

# - Vision + Language Transformers
### CLIP: connecting text and images
- CLIP : contrastive language-image pre-training, by OpenAI
- Architecture
    - Image encoder : ViT-B(or ResNet 50)
    - Text encoder : Transformers
- Contrastive learing
- Pre-training method for CLIP

### Application 1 : image captioning
- ZeroCap : zero-shot image-to-text generation for visual-semantic arithmetic
    - Background of GPT-2 
    - Zero-shot results

### Application 2 : image stylization
- StyleCLIP : text-driven manipulation of StyleGAN imagery
- Method
    - Mapping network is trained for a specific text prompt

### Applictaion 3 : video retrieval
- CLIP4Clip : an empirical study of CLIP for end-to-end video clip retrieval

### Application 4 : text-to-image generation
- DALL-E^2 : Hierarchical Text-Conditional Image Generation with CLIP Latents

### Application 5 : 4D human generation
- CLIP-Actor : Text-driven Recommendation and Stylization For Animating Human 


# - Unified Transformers
- Single-purpose models
- Multi-purpose models

### OMNIVORE
- Retrieval results across all modalities

### UniT : Mutimodal Multitask Learning with a Unified Transformer

### GPV-1 : general purpose models
- an end-to-end task-agnostic vision-language architecture

### Unified-IO
- A Unified Model for Vision, Language, and Multi-modal Tasks

### Few-shot learning for various tasks
- Flamingo : a Visual Language Model for Few-Shot Learning

# - Summary
- Transformer captures long-term dependency attention
- Transformer is applied to various computer vision tasks
- Trend towards bridging the vision and language modalities by transformer
- Trend towards designing the unified transformer
