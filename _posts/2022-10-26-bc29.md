---
layout: single
title:  "Week6-day3"
toc: true
toc_sticky: true

use_math: true
---

CV 기초대회 - Model - Training & Inference & Process(train, inference)

<br>

## [ special ]


# - Loss

### Error Backpropagation
- loss.bsckward()
    - 함수가 실행되면 모델의 파라미터의 grad값이 업데이트 된다
- Focal Loss
- Lable Smoothing Loss

# - Optimizer

### LR scheduler

- 변화를 주면서 찾아주게 되면 더 효율적
- StepLR
    - 특정 Step마다 LR감소
    - ex)scheduler = torch.optim.lr_scheduler.SterpLR(optimzier, step_size=2, gamma=0.1)
- CosineAnnealingLR
    - Cosine 함수 형태처럼 LR을 급격히 변경
    - ex)scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimzier, T_max=10, eta_min=0)
- ReduceLROnPlateau
    - 더 이상 성능 향상이 없을 때 LR 감소
    - ex)scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimzier, 'min')
        
# - Metric

### 모델의 평가
- Classification
    - Accuracy, F1-score, precision, recall, AUC-ROC
- Regression
    - MAE, MSE
- Ranking
    - MRR, NDCG, MAP
    
# - Training Process
- model.train()
- optimizer.zero_grad()
    - 사용이유 : 이전단계의 배치의 grad가 그대로 남아있기 때문
- loss = criterion(outputs, labels)
    - loss를 마지막으로 chain 생성 
    - loss의 grad_fn chain -> loss.backward()
- optimizer.step()

# - Inference Process
- 검증의 단계
- model.eval()
- with torch.no_grad() :
    - 안에 내용은 모두 no grad
- Validation 확인
- Checkpoint
- 최종 Output, Submission 형태로 변환

# - Appendix : Pytorch Lightning

