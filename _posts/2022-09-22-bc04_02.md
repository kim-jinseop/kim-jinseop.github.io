---
layout: single
title:  "Week1-day4-2."

use_math: true
---


# AI-math

<br>   

## 1. 벡터
- 숫자를 원소로 가지는 리스트 또는 배열
- 공간에서 한 점 

### 벡터의 덧셈, 뺄셈
- 두 벡터의 덧셈은 다른 벡터로부터 상대적 `위치이동`을 표현

### 노름
- 백터의 노름은 원점에서부터의 거리
- L1 노름은 `각 성분의 변화량의 절대값`을 모두 더한다

    $\begin{Vmatrix} x \end{Vmatrix}_1 = \displaystyle\sum_{i=0}^{\infty}\begin{Vmatrix} x_i \end{Vmatrix}$



- L2 노름은 피타고라스 정리를 이용해 `유클리드 거리`를 계산

    $\left \| x \right \|_2 = \sqrt{\displaystyle\sum_{i=0}^{\infty}\left | x_i \right|^2}$

### 두 벡터 사이의 거리
- L2 노름을 이용해 구할 수 있다.


### 두 벡터 사이의 각도
- 제2 코사인 법칙으로 두 벡터 사이의 각도를 계산할 수 있다.
  
    $\cos \theta = \frac{\left \| x \right \|_2^2 + \left \| y \right \|_2^2 - \left \| x-y \right \|_2^2}{2\left \| x \right \|_2\left \| y \right \|_2}$

- 분자를 쉽게 계산하는 방법이 내적이다.

    $\cos \theta = \frac{\left \langle x,y \right \rangle}{\left \| x \right \|_2\left \| y \right \|_2}$

    $\left \langle x,y \right \rangle = \displaystyle\sum_{i=1}^{d}x_iy_i$

### 내적
- 정사영된 벡터의 길이와 관련있다.
- Proj(x)의 길이는 코사인법칙에 의해 $ \left \| x \right \|\cos \theta $

<br>

## 2. 행렬
- 행렬은 벡터를 원소로 가지는 2차원 배열
- 행렬은 행과 열이라는 인덱스를 가진다.
- 행렬의 $x_{ij}$는 i번째 데이터의 j번째 변수의 값을 말한다.
- 행렬곱을 통해 패턴을 추출할 수 있고, 데이터를 압축할 수도 있다.
- 행렬은 벡터공간에서 사용되는 연산자로 이해한다.

### 행렬 곱셈
- i번째 행벡터와 i번째 열벡터 사이의 내적을 성분으로 가지는 행렬을 계산
$XY = \left ( \sum_k x_{ik}y_{kj} \right )$

- 주의 : 넘파이의 np.inner는 i번째 행벡터와 j번째 열벡터 사이의 내적으로 성분으로 가지는 행렬을 계산

### 역행렬
- 어떤 행렬 A의 연산을 거꾸로 되들리는 행열을 역행이라 한다. $A^{-1}$  로 표시
- 만일 역행렬을 계산할 수 없다면 유사역행렬 또는 무어-펜로즈 역행렬 $A^+$ 을 이용한다

- 응용1 : 연립방정식의 해를 구할 수 있다.
- 응용2 : 데이터를 선형모델로 해석하는 선형회귀식을 찾을 수 있다.

<br>

## 3. 경사하강법 part1

### 미분
- 미분은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구
- 미분은 함수 f의 주어진 점(x, f(x)) 에서의 `접선의 기울기`를 구한다 
- 미분값을 더하면 `경사 상승법`이라 하며 함수의 `극대값`의 위치를 구할때 사용 
- 미분값을 빼면 `경사 하강법`이라 하며 함수의 `극소값`의 위치를 구할때 사용 

    $f'(x)= \lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{h}$

#### 편미분
- 벡터가 입력인 다변수 함수의 경우 `편미분`을 사용
- 각 변수 별로 편미분을 계산한 `그레디언트 벡터`를 이용하여 경사하강/경사상승법에 사용할 수 있다

<br>

## 4. 경사하강법 part2

### 경사하강법으로 선형회귀 계수 구하기

- 선형회귀의 목적식은 $∥y − Xβ∥_2$ 이고 이를 최소화하는 $β$를 찾아야 하므로 다음과 같은 그레디언트 벡터를 구해야한다  

    $= \frac{X^T(y-X\beta)}{∥y − Xβ∥_2}$

목적식을 최소하화는 B를 구하는 경사하강법 알고리즘 :

$\beta^{(t+1)} \leftarrow \beta^{(t)}  + \frac{2\lambda }{n}X^T(y-X\beta^{(t)})$


#### @ 경사하강법!
- 미분가능하고 볼록한 함수에 대해선 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장
- 비선형회귀 문제의 경우 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지 않는다 

### 확률적 경사하강법(stochastic gradient decent) SGD 
- 모든 데이터를 사용해서 업데이트하는 대신 데이터 한개 또는 일부 활용하여 업데이트한다.
- SGD는 데이터의 일부를 가지고 패러미터를 업데이트하기 때문에 연산자원을 좀 더 효율적으로 활용하는데 도움이 된다
    - 머신러닝 학습에 더 효율적이다 
    
    
## `학습률, 학습횟수를 잘 조절을 해야한다`
